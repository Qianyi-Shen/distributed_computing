{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecf8680-5306-4cef-a274-aa088e889945",
   "metadata": {},
   "source": [
    "### Horovod on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fe0b6-1b85-4d39-befc-f05691c1b52c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Source: https://horovod.readthedocs.io/en/stable/spark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9950fe-365c-4a3e-bf00-6dc3365e7208",
   "metadata": {},
   "source": [
    "**PURPOSE:**  \n",
    "Demo of distributed model training using Horovod with Spark.\n",
    "\n",
    "The Estimator API abstracts the data processing (from Spark DataFrames to deep learning datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d768c105-de08-40cc-92cd-2c57f1e1cb45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: horovod[spark] in /home/apt4c/.local/lib/python3.7/site-packages (0.28.1)\n",
      "Requirement already satisfied: cloudpickle in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (2.0.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (6.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (23.1)\n",
      "Requirement already satisfied: cffi>=1.4.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (1.15.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (1.21.6)\n",
      "Requirement already satisfied: petastorm>=0.12.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (0.12.1)\n",
      "Requirement already satisfied: pyarrow<11.0,>=0.15.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (10.0.1)\n",
      "Requirement already satisfied: fsspec>=2021.07.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (2023.1.0)\n",
      "Requirement already satisfied: pyspark>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (3.3.1)\n",
      "Requirement already satisfied: pycparser in /home/apt4c/.local/lib/python3.7/site-packages (from cffi>=1.4.0->horovod[spark]) (2.21)\n",
      "Requirement already satisfied: dill>=0.2.1 in /home/apt4c/.local/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (0.3.7)\n",
      "Requirement already satisfied: diskcache>=3.0.0 in /home/apt4c/.local/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (5.6.3)\n",
      "Requirement already satisfied: future>=0.10.2 in /home/apt4c/.local/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (1.0.0)\n",
      "Requirement already satisfied: pandas>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (1.3.5)\n",
      "Requirement already satisfied: pyzmq>=14.0.0 in /opt/conda/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (24.0.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (1.16.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark>=2.3.2->horovod[spark]) (0.10.9.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.19.0->petastorm>=0.12.0->horovod[spark]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.19.0->petastorm>=0.12.0->horovod[spark]) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install horovod[spark]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a250f-1161-4737-b94e-3e5c46dfc715",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354fadbb-3030-4083-b7c2-ed1aa6b80075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import horovod.spark.keras as hvd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107eaf3-8cd5-4db4-be31-bd752eef9d3c",
   "metadata": {},
   "source": [
    "Import data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2e0993-ef1f-4e2f-a08c-b0d81bf0fcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/sfs/gpfs/tardis/home/apt4c/distributed_computing/04_mllib_intro_and_supervised_learning/'\n",
    "DATA_FILENAME = 'wisc_breast_cancer_w_fields.csv'\n",
    "DATA_FILEPATH = os.path.join(DATA_DIR, DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1e71a7-1a7a-4033-b5c1-6538f1188097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/24 18:42:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c47486f-aa3e-4ab7-8d9f-598f9da55053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(DATA_FILEPATH, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5831a6c5-724b-4fd8-809e-caf5f4963094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diag_ind = when(col(\"diagnosis\") == 'M', 1).otherwise(0)\n",
    "df = df.withColumn(\"y\", diag_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5daa3567-7e23-40d4-9066-c183dbbe99aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.6, 0.4], seed = 314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4737a50-48a8-429f-bce9-06ebf8821045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\"], outputCol=\"features\")\n",
    "train_df = assembler.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8414a4-2200-4539-bcdd-1edd7b9a01c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/24 18:43:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+---+-------------+\n",
      "|  y|     features|\n",
      "+---+-------------+\n",
      "|  1|[15.46,19.48]|\n",
      "|  0|[12.89,13.12]|\n",
      "|  0| [14.96,19.1]|\n",
      "|  1|[13.17,18.66]|\n",
      "|  0|[12.18,17.84]|\n",
      "|  1|[22.27,19.67]|\n",
      "|  1|[18.66,17.12]|\n",
      "|  0|[11.15,13.08]|\n",
      "|  0|  [10.8,9.71]|\n",
      "|  1|[13.43,19.63]|\n",
      "+---+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.select('y', 'features').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328d2bd-92d5-4c44-9f34-b845c497d6da",
   "metadata": {},
   "source": [
    "Set up neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71245fed-14fe-4dc6-ad2c-68f5b5a6f612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, activation = 'tanh', input_dim=2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807135de-f530-40aa-ace3-f58198b6cbea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: unscaled learning rate\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss = 'binary_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508911f7-bf5f-4e41-bc15-1a26794bf696",
   "metadata": {},
   "source": [
    "Set up intermediate storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "810b9c57-8894-4e96-be28-fafce59e8394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from horovod.spark.common.store import Store\n",
    "\n",
    "store = Store.create('/tmp/horovod/experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538aa992-b9d5-4106-bdeb-ce79473c27a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_estimator = hvd.KerasEstimator(\n",
    "    num_proc=2,\n",
    "    model=model,\n",
    "    store=store,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    feature_cols=['features'],\n",
    "    label_cols=['y'],\n",
    "    batch_size=32,\n",
    "    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec6c588-c666-4dae-aec5-bb72f804a8e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_partitions=10\n",
      "writing dataframes\n",
      "train_data_path=file:///tmp/horovod/experiment/intermediate_train_data.0\n",
      "val_data_path=file:///tmp/horovod/experiment/intermediate_val_data.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_partitions=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rows=354\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/common/util.py:495: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  train_data_schema = train_data.schema.to_arrow_schema()\n",
      "/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/common/util.py:405: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  for piece in dataset.pieces:\n",
      "/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/common/util.py:513: FutureWarning: The 'field_by_name' method is deprecated, use 'field' instead\n",
      "  metadata, avg_row_size = make_metadata_dictionary(train_data_schema)\n",
      "2024-10-24 14:39:46.276757: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-10-24 14:39:46.357502: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000010000 Hz\n",
      "2024-10-24 14:39:46.357810: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c65e1c9f80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-24 14:39:46.358466: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "[0]<stdout>:Pinning current process to the GPU.                     (0 + 1) / 1]\n",
      "[0]<stderr>:WARNING:tensorflow:From /home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/keras/remote.py:322: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "[0]<stderr>:\n",
      "[0]<stderr>:WARNING:tensorflow:From /home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/keras/remote.py:326: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "[0]<stderr>:\n",
      "[0]<stderr>:WARNING:tensorflow:From /home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/keras/remote.py:326: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "[0]<stderr>:\n",
      "[0]<stderr>:2024-10-24 14:39:48.678539: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "[0]<stderr>:2024-10-24 14:39:48.694491: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000010000 Hz\n",
      "[0]<stderr>:2024-10-24 14:39:48.694622: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bd0c119c70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "[0]<stderr>:2024-10-24 14:39:48.694695: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "[0]<stderr>:Traceback (most recent call last):\n",
      "[0]<stderr>:  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "[0]<stderr>:    \"__main__\", mod_spec)\n",
      "[0]<stderr>:  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "[0]<stderr>:    exec(code, run_globals)\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/task/gloo_exec_fn.py\", line 30, in <module>\n",
      "[0]<stderr>:    main(codec.loads_base64(sys.argv[1]), codec.loads_base64(sys.argv[2]))\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/task/gloo_exec_fn.py\", line 23, in main\n",
      "[0]<stderr>:    task_exec(driver_addresses, settings, 'HOROVOD_RANK', 'HOROVOD_LOCAL_RANK')\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/task/__init__.py\", line 61, in task_exec\n",
      "[0]<stderr>:    result = fn(*args, **kwargs)\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/keras/remote.py\", line 137, in train\n",
      "[0]<stderr>:    serialized_model, lambda x: hvd.load_model(x))\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/keras/remote.py\", line 299, in deserialize_keras_model\n",
      "[0]<stderr>:    return load_model_fn(f)\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/keras/remote.py\", line 137, in <lambda>\n",
      "[0]<stderr>:    serialized_model, lambda x: hvd.load_model(x))\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/tensorflow/keras/__init__.py\", line 268, in load_model\n",
      "[0]<stderr>:    return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects, legacy_opts)\n",
      "[0]<stderr>:  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/_keras/__init__.py\", line 316, in load_model\n",
      "[0]<stderr>:    return keras.models.load_model(filepath, custom_objects=horovod_objects)\n",
      "[0]<stderr>:  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 143, in load_model\n",
      "[0]<stderr>:    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\n",
      "[0]<stderr>:  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 160, in load_model_from_hdf5\n",
      "[0]<stderr>:    model_config = json.loads(model_config.decode('utf-8'))\n",
      "[0]<stderr>:AttributeError: 'str' object has no attribute 'decode'\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/runner.py\", line 142, in run_spark\n",
      "    result = procs.mapPartitionsWithIndex(mapper).collect()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\", line 1197, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job 6 cancelled part of cancelled job group horovod.spark.run.0\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2554)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1151)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1150)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2822)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Horovod detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:\nProcess name: 0\nExit code: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_782946/278569878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetOutputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/common/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, params)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mHorovodModel\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mwrapping\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHorovodEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_on_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/common/estimator.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_metadata_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             return self._fit_on_prepared_data(\n\u001b[0;32m---> 81\u001b[0;31m                 backend, train_rows, val_rows, metadata, avg_row_size, dataset_idx)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_or_create_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/keras/estimator.py\u001b[0m in \u001b[0;36m_fit_on_prepared_data\u001b[0;34m(self, backend, train_rows, val_rows, metadata, avg_row_size, dataset_idx)\u001b[0m\n\u001b[1;32m    291\u001b[0m         handle = backend.run(trainer,\n\u001b[1;32m    292\u001b[0m                              \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_row_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                              env=self.getBackendEnv())\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/common/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fn, args, kwargs, env)\u001b[0m\n\u001b[1;32m     83\u001b[0m         return horovod.spark.run(fn, args=args, kwargs=kwargs,\n\u001b[1;32m     84\u001b[0m                                  \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_proc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                                  **self._kwargs)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fn, args, kwargs, num_proc, start_timeout, use_mpi, use_gloo, extra_mpi_args, env, stdout, stderr, verbose, nics, prefix_output_with_timestamp, executable)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# Run the job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0m_launch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gloo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# Terminate Spark job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/runner.py\u001b[0m in \u001b[0;36m_launch_job\u001b[0;34m(use_mpi, use_gloo, settings, driver, env, stdout, stderr, executable)\u001b[0m\n\u001b[1;32m    156\u001b[0m                    \u001b[0muse_mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmpi_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                    \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                    settings.verbose)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/runner/launch.py\u001b[0m in \u001b[0;36mrun_controller\u001b[0;34m(use_gloo, gloo_run, use_mpi, mpi_run, use_jsrun, js_run, verbosity)\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0mmpi_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgloo_built\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mgloo_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             raise ValueError('Neither MPI nor Gloo support has been built. Try reinstalling Horovod ensuring that '\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/runner.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mnics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_common_interfaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     run_controller(use_gloo, lambda: gloo_run(executable, settings, nics, driver, env, stdout, stderr),\n\u001b[0m\u001b[1;32m    156\u001b[0m                    \u001b[0muse_mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmpi_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                    \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/spark/gloo_run.py\u001b[0m in \u001b[0;36mgloo_run\u001b[0;34m(executable, settings, nics, driver, env, stdout, stderr)\u001b[0m\n\u001b[1;32m     66\u001b[0m     exec_command = _exec_command_fn(driver, key, settings, env,\n\u001b[1;32m     67\u001b[0m                                     stdout, stderr, settings.prefix_output_with_timestamp)\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mlaunch_gloo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_ip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/horovod/runner/gloo_run.py\u001b[0m in \u001b[0;36mlaunch_gloo\u001b[0;34m(command, exec_command, settings, nics, env, server_ip)\u001b[0m\n\u001b[1;32m    285\u001b[0m                                \u001b[0;34m'status, thus causing the job to be terminated. The first process '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                                \u001b[0;34m'to do so was:\\nProcess name: {name}\\nExit code: {code}\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                                .format(name=name, code=exit_code))\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Horovod detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:\nProcess name: 0\nExit code: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/24 18:39:52 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6) (udc-an28-18 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "keras_model = keras_estimator.fit(train_df) \\\n",
    "    .setOutputCols(['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0b4956-54b0-4ef5-8e49-1d7775a62321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_782946/3290922864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'keras_model' is not defined"
     ]
    }
   ],
   "source": [
    "predict_df = keras_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13250d3b-9b49-491c-bcc7-55cc2b483054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
